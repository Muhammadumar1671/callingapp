{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping for Auto Repair Shops in Alabama\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 229\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# Step 1: Scrape Google Maps business links\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m business_links \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_business_links\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# Initialize a list to store data for this category and state\u001b[39;00m\n\u001b[1;32m    232\u001b[0m data_to_save \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[3], line 114\u001b[0m, in \u001b[0;36mscrape_business_links\u001b[0;34m(category, state)\u001b[0m\n\u001b[1;32m    111\u001b[0m search_box\u001b[38;5;241m.\u001b[39msend_keys(Keys\u001b[38;5;241m.\u001b[39mENTER)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Wait for the page to load results\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Scroll down the list\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re\n",
    "import requests\n",
    "import html\n",
    "from urllib3.exceptions import IncompleteRead\n",
    "# Initialize the Chrome driver with options\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--disable-extensions\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--hide-scrollbars\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument('--headless')  # Uncomment this line if you want to run in headless mode\n",
    "options.add_argument(\"--ignore-certificate-errors\")\n",
    "options.add_argument(\"--disable-session-crashed-bubble\")\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option('useAutomationExtension', False)\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "options.add_argument(\"--disable-webgl\")  # Disable WebGL\n",
    "options.add_argument(\"--disable-gpu\")  # Ensure GPU acceleration is off\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# driver_path = r\"c:\\Program Files (x86)\\chromedriver-win64\\chromedriver.exe\"  # Path to chromedriver executable\n",
    "# service = Service(executable_path=driver_path)\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "# Define the categories and states\n",
    "categories = [\n",
    "    # \"Real Estate Agents (Independent)\", \n",
    "    # \"Home Services (Landscaping, Cleaning, Pest Control)\", \n",
    "    # \"Event Planning and Catering\", \n",
    "    # \"Professional Services (Legal, Accounting, Financial Advisors)\", \n",
    "    \"Auto Repair Shops\", \n",
    "    \"Local Retail Stores shopify stores\", \n",
    "    \"Construction and Contracting\", \n",
    "    \"Jewelry Stores (Custom, High-End, Vintage)\", \n",
    "    \"Antique and Collectible Shops\"\n",
    "]\n",
    "states = [\n",
    "    \"Alabama\",\n",
    "    \"Alaska\",\n",
    "    \"Arizona\",\n",
    "    \"Arkansas\",\n",
    "    \"California\",\n",
    "    \"Colorado\",\n",
    "    \"Connecticut\",\n",
    "    \"Delaware\",\n",
    "    \"Florida\",\n",
    "    \"Georgia\",\n",
    "    \"Hawaii\",\n",
    "    \"Idaho\",\n",
    "    \"Illinois\",\n",
    "    \"Indiana\",\n",
    "    \"Iowa\",\n",
    "    \"Kansas\",\n",
    "    \"Kentucky\",\n",
    "    \"Louisiana\",\n",
    "    \"Maine\",\n",
    "    \"Maryland\",\n",
    "    \"Massachusetts\",\n",
    "    \"Michigan\",\n",
    "    \"Minnesota\",\n",
    "    \"Mississippi\",\n",
    "    \"Missouri\",\n",
    "    \"Montana\",\n",
    "    \"Nebraska\",\n",
    "    \"Nevada\",\n",
    "    \"New Hampshire\",\n",
    "    \"New Jersey\",\n",
    "    \"New Mexico\",\n",
    "    \"New York\",\n",
    "    \"North Carolina\",\n",
    "    \"North Dakota\",\n",
    "    \"Ohio\",\n",
    "    \"Oklahoma\",\n",
    "    \"Oregon\",\n",
    "    \"Pennsylvania\",\n",
    "    \"Rhode Island\",\n",
    "    \"South Carolina\",\n",
    "    \"South Dakota\",\n",
    "    \"Tennessee\",\n",
    "    \"Texas\",\n",
    "    \"Utah\",\n",
    "    \"Vermont\",\n",
    "    \"Virginia\",\n",
    "    \"Washington\",\n",
    "    \"West Virginia\",\n",
    "    \"Wisconsin\",\n",
    "    \"Wyoming\"\n",
    "]\n",
    "\n",
    "# Function to perform the Google Maps search and scrape links\n",
    "def scrape_business_links(category, state):\n",
    "    try:\n",
    "        search_query = f\"{category} in {state}, Africa\"\n",
    "        driver.get(\"https://www.google.com/maps\")\n",
    "\n",
    "        # Input search query\n",
    "        search_box = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.NAME, \"q\")))\n",
    "        search_box.send_keys(search_query)\n",
    "        search_box.send_keys(Keys.ENTER)\n",
    "\n",
    "        # Wait for the page to load results\n",
    "        time.sleep(10)\n",
    "\n",
    "        # Scroll down the list\n",
    "        try:\n",
    "            scrollable_div = driver.find_element(By.CSS_SELECTOR, 'div[role=\"feed\"]')\n",
    "            driver.execute_script(\"\"\"\n",
    "                var scrollableDiv = arguments[0];\n",
    "                function scrollWithinElement(scrollableDiv) {\n",
    "                    return new Promise((resolve, reject) => {\n",
    "                        var totalHeight = 0;\n",
    "                        var distance = 1000;\n",
    "                        var scrollDelay = 5000;\n",
    "                        \n",
    "                        var timer = setInterval(() => {\n",
    "                            var scrollHeightBefore = scrollableDiv.scrollHeight;\n",
    "                            scrollableDiv.scrollBy(0, distance);\n",
    "                            totalHeight += distance;\n",
    "\n",
    "                            if (totalHeight >= scrollHeightBefore) {\n",
    "                                totalHeight = 0;\n",
    "                                setTimeout(() => {\n",
    "                                    var scrollHeightAfter = scrollableDiv.scrollHeight;\n",
    "                                    if (scrollHeightAfter > scrollHeightBefore) {\n",
    "                                        return;\n",
    "                                    } else {\n",
    "                                        clearInterval(timer);\n",
    "                                        resolve();\n",
    "                                    }\n",
    "                                }, scrollDelay);\n",
    "                            }\n",
    "                        }, 200);\n",
    "                    });\n",
    "                }\n",
    "                return scrollWithinElement(scrollableDiv);\n",
    "            \"\"\", scrollable_div)\n",
    "        except Exception as e:\n",
    "            print(f\"Oops! Something went wrong while scrolling: {e}\")\n",
    "        \n",
    "        # Parse the page source with BeautifulSoup\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Find all links starting with \"https://www.google.com/\"\n",
    "        google_links = [link.get('href') for link in soup.find_all('a', href=True) if link.get('href').startswith('https://www.google.com/')]\n",
    "        return google_links\n",
    "    except Exception as e:\n",
    "        print(f\"Error while scraping links for {category} in {state}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to check if website is valid\n",
    "def check_u003dhttp(html):\n",
    "    try:\n",
    "        pattern = r'com\\\\\",null,null,\\\\\"'  # Updated pattern\n",
    "        if re.search(pattern, html):\n",
    "            return \"yes\"\n",
    "        else:\n",
    "            return \"no\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error in checking website existence: {e}\")\n",
    "        return \"no\"\n",
    "\n",
    "# Function to extract phone numbers, names, and addresses\n",
    "def extract_info(url):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise an exception for 4xx or 5xx status codes\n",
    "        html_content = response.text\n",
    "        \n",
    "        # Extract phone numbers\n",
    "        phone_numbers = re.findall(r'tel:\\+(\\d+)', html_content)\n",
    "        if not phone_numbers:\n",
    "            phone_numbers = re.findall(r'tel:(\\d+)', html_content)\n",
    "            # Remove any non-numeric characters from phone numbers\n",
    "            phone_numbers = [re.sub(r'\\D', '', number) for number in phone_numbers]\n",
    "        \n",
    "        # Extract name and address\n",
    "        name_matches = re.findall(r'<meta content=\"([^\"]+)\" itemprop=\"name\">', html_content)        \n",
    "        name = html.unescape(name_matches[0]) if name_matches else None\n",
    "        website_existence = check_u003dhttp(html_content)\n",
    "        \n",
    "        # Extract address from name\n",
    "        address = None\n",
    "        if name:\n",
    "            address_matches = re.findall(r'·([^·]+)$', name.strip())\n",
    "            address = address_matches[0].strip() if address_matches else None\n",
    "            name = name.split(\"·\")[0].strip() if address else name\n",
    "        # Remove commas from the address\n",
    "        if address:\n",
    "            address = address.replace(',', '')\n",
    "        return phone_numbers, name, address, website_existence\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request error while fetching the page: {url} - {e}\")\n",
    "    except (IncompleteRead, re.error) as e:\n",
    "        print(f\"Parsing error occurred for the page: {url} - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"General error occurred: {e}\")\n",
    "    return None, None, None, None\n",
    "\n",
    "# Function to save results to CSV\n",
    "def save_to_csv(filename, data):\n",
    "    try:\n",
    "        with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerows(data)\n",
    "    except IOError as e:\n",
    "        print(f\"Oops! Couldn't save the file. Error: {e}\")\n",
    "\n",
    "# Main scraping and extraction loop\n",
    "for category in categories:\n",
    "    for state in states:\n",
    "        try:\n",
    "            print(f\"Scraping for {category} in {state}\")\n",
    "            \n",
    "            # Step 1: Scrape Google Maps business links\n",
    "            business_links = scrape_business_links(category, state)\n",
    "            \n",
    "            # Initialize a list to store data for this category and state\n",
    "            data_to_save = []\n",
    "            \n",
    "            # Step 2: Extract information from each link\n",
    "            for url in business_links:\n",
    "                phone_numbers, name, address, website = extract_info(url)\n",
    "                if phone_numbers or name or address or website:\n",
    "                    data_to_save.append([phone_numbers, name, address, website, category])\n",
    "                    print(f\"{phone_numbers}, {name}, {address}, {website}, {category}\")\n",
    "            \n",
    "            # Step 3: Save the extracted data to CSV after each category and state iteration\n",
    "            save_to_csv('business_links_and_info.csv', data_to_save)\n",
    "            \n",
    "            print(f\"Saved {len(data_to_save)} entries for {category} in {state}\")\n",
    "            \n",
    "            # Optional delay to avoid rate-limiting\n",
    "            time.sleep(5)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in main loop for {category} in {state}: {e}\")\n",
    "\n",
    "# Close the browser when done\n",
    "driver.quit()\n",
    "\n",
    "print(\"Scraping and data extraction completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re\n",
    "import requests\n",
    "import html\n",
    "from urllib3.exceptions import IncompleteRead\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "class BusinessScraper:\n",
    "    def __init__(self):\n",
    "        self.driver = self._setup_driver()\n",
    "\n",
    "    def _setup_driver(self) -> webdriver.Chrome:\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"--disable-extensions\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--hide-scrollbars\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument(\"--ignore-certificate-errors\")\n",
    "        options.add_argument(\"--disable-session-crashed-bubble\")\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument(\"--start-maximized\")\n",
    "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        options.add_experimental_option('useAutomationExtension', False)\n",
    "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        options.add_argument(\"--disable-webgl\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        \n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        return webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    def scrape_businesses(self, category: str, state: str, output_file: str = 'business_links_and_info.csv') -> List[List]:\n",
    "        \"\"\"\n",
    "        Main function to scrape business information for a given category and state\n",
    "        Returns list of [phone_numbers, name, address, website, category]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Scraping for {category} in {state}\")\n",
    "            business_links = self._scrape_business_links(category, state)\n",
    "            data = self._process_links(business_links, category)\n",
    "            self._save_to_csv(output_file, data)\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error while scraping {category} in {state}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _scrape_business_links(self, category: str, state: str) -> List[str]:\n",
    "        try:\n",
    "            search_query = f\"{category} in {state}, USA\"\n",
    "            self.driver.get(\"https://www.google.com/maps\")\n",
    "\n",
    "            # Input search query\n",
    "            search_box = WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.NAME, \"q\")))\n",
    "            search_box.send_keys(search_query)\n",
    "            search_box.send_keys(Keys.ENTER)\n",
    "\n",
    "            # Wait for the page to load results\n",
    "            time.sleep(10)\n",
    "\n",
    "            # Scroll down the list\n",
    "            try:\n",
    "                scrollable_div = self.driver.find_element(By.CSS_SELECTOR, 'div[role=\"feed\"]')\n",
    "                self.driver.execute_script(\"\"\"\n",
    "                    var scrollableDiv = arguments[0];\n",
    "                    function scrollWithinElement(scrollableDiv) {\n",
    "                        return new Promise((resolve, reject) => {\n",
    "                            var totalHeight = 0;\n",
    "                            var distance = 1000;\n",
    "                            var scrollDelay = 5000;\n",
    "                            \n",
    "                            var timer = setInterval(() => {\n",
    "                                var scrollHeightBefore = scrollableDiv.scrollHeight;\n",
    "                                scrollableDiv.scrollBy(0, distance);\n",
    "                                totalHeight += distance;\n",
    "\n",
    "                                if (totalHeight >= scrollHeightBefore) {\n",
    "                                    totalHeight = 0;\n",
    "                                    setTimeout(() => {\n",
    "                                        var scrollHeightAfter = scrollableDiv.scrollHeight;\n",
    "                                        if (scrollHeightAfter > scrollHeightBefore) {\n",
    "                                            return;\n",
    "                                        } else {\n",
    "                                            clearInterval(timer);\n",
    "                                            resolve();\n",
    "                                        }\n",
    "                                    }, scrollDelay);\n",
    "                                }\n",
    "                            }, 200);\n",
    "                        });\n",
    "                    }\n",
    "                    return scrollWithinElement(scrollableDiv);\n",
    "                \"\"\", scrollable_div)\n",
    "            except Exception as e:\n",
    "                print(f\"Oops! Something went wrong while scrolling: {e}\")\n",
    "            \n",
    "            # Parse the page source with BeautifulSoup\n",
    "            page_source = self.driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "            # Find all links starting with \"https://www.google.com/\"\n",
    "            google_links = [link.get('href') for link in soup.find_all('a', href=True) if link.get('href').startswith('https://www.google.com/')]\n",
    "            return google_links\n",
    "        except Exception as e:\n",
    "            print(f\"Error while scraping links for {category} in {state}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _extract_info(self, url: str) -> Tuple[Optional[List[str]], Optional[str], Optional[str], Optional[str]]:\n",
    "        try:\n",
    "            headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"}\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()  # Raise an exception for 4xx or 5xx status codes\n",
    "            html_content = response.text\n",
    "            \n",
    "            # Extract phone numbers\n",
    "            phone_numbers = re.findall(r'tel:\\+(\\d+)', html_content)\n",
    "            if not phone_numbers:\n",
    "                phone_numbers = re.findall(r'tel:(\\d+)', html_content)\n",
    "                # Remove any non-numeric characters from phone numbers\n",
    "                phone_numbers = [re.sub(r'\\D', '', number) for number in phone_numbers]\n",
    "            \n",
    "            # Extract name and address\n",
    "            name_matches = re.findall(r'<meta content=\"([^\"]+)\" itemprop=\"name\">', html_content)        \n",
    "            name = html.unescape(name_matches[0]) if name_matches else None\n",
    "            website_existence = self._check_u003dhttp(html_content)\n",
    "            \n",
    "            # Extract address from name\n",
    "            address = None\n",
    "            if name:\n",
    "                address_matches = re.findall(r'·([^·]+)$', name.strip())\n",
    "                address = address_matches[0].strip() if address_matches else None\n",
    "                name = name.split(\"·\")[0].strip() if address else name\n",
    "            # Remove commas from the address\n",
    "            if address:\n",
    "                address = address.replace(',', '')\n",
    "            return phone_numbers, name, address, website_existence\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Request error while fetching the page: {url} - {e}\")\n",
    "        except (IncompleteRead, re.error) as e:\n",
    "            print(f\"Parsing error occurred for the page: {url} - {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"General error occurred: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    def _check_u003dhttp(self, html: str) -> str:\n",
    "        try:\n",
    "            pattern = r'com\\\\\",null,null,\\\\\"'  # Updated pattern\n",
    "            if re.search(pattern, html):\n",
    "                return \"yes\"\n",
    "            else:\n",
    "                return \"no\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error in checking website existence: {e}\")\n",
    "            return \"no\"\n",
    "\n",
    "    def _process_links(self, links: List[str], category: str) -> List[List]:\n",
    "        data = []\n",
    "        for url in links:\n",
    "            phone_numbers, name, address, website = self._extract_info(url)\n",
    "            if any([phone_numbers, name, address, website]):\n",
    "                data.append([phone_numbers, name, address, website, category])\n",
    "                print(f\"Found: {phone_numbers}, {name}, {address}, {website}, {category}\")\n",
    "        return data\n",
    "\n",
    "    def _save_to_csv(self, filename: str, data: List[List]):\n",
    "        try:\n",
    "            with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerows(data)\n",
    "        except IOError as e:\n",
    "            print(f\"Error saving to CSV: {e}\")\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the browser when done\"\"\"\n",
    "        self.driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # Initialize the scraper\n",
    "    scraper = BusinessScraper()\n",
    "    \n",
    "    try:\n",
    "        # Example usage\n",
    "        category = \"Auto Repair Shops\"\n",
    "        state = \"California\"\n",
    "        \n",
    "        # Scrape businesses and get results\n",
    "        results = scraper.scrape_businesses(category, state)\n",
    "        \n",
    "        # Process results as needed\n",
    "        print(f\"Found {len(results)} businesses\")\n",
    "        \n",
    "    finally:\n",
    "        # Always close the browser\n",
    "        scraper.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
